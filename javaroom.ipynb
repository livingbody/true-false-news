{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、基于PaddleNLP的美国大选的新闻真假分类（二）基于SKEP模型的分类任务\n",
    "\n",
    "## 0.解释\n",
    "**本来这个都烂尾了，看到有人问二在哪儿？只好说还没公开，自己挖的坑，含泪也要填。下次标题再也不屑一、二了，真的很容易烂尾。。。。。。**\n",
    "\n",
    "## 1.简介\n",
    "新闻媒体已成为向世界人民传递世界上正在发生的事情的信息的渠道。 人们通常认为新闻中传达的一切都是真实的。 在某些情况下，甚至新闻频道也承认他们的新闻不如他们写的那样真实。 但是，一些新闻不仅对人民或政府产生重大影响，而且对经济也产生重大影响。 一则新闻可以根据人们的情绪和政治局势上下移动曲线。\n",
    "\n",
    "从真实的真实新闻中识别虚假新闻非常重要。 该问题已通过自然语言处理工具解决并得到了解决，本篇文章可帮助我们根据历史数据识别假新闻或真实新闻。\n",
    "\n",
    "## 2.问题描述\n",
    "对于印刷媒体和数字媒体，信息的真实性已成为影响企业和社会的长期问题。在社交网络上，信息传播的范围和影响以如此快的速度发生，并且如此迅速地放大，以至于失真，不准确或虚假的信息具有巨大的潜力，可在数分钟内对数百万用户造成现实世界的影响。最近，人们表达了对该问题的一些担忧，并提出了一些缓解该问题的方法。\n",
    "\n",
    "在各种信息广播的整个历史中，一直存在着不那么精确的引人注目和引人入胜的新闻标题，这些新闻标题旨在吸引观众的注意力来出售信息。但是，在社交网站上，信息传播的范围和影响得到了显着放大，并且发展速度如此之快，以至于失真，不准确或虚假的信息具有巨大的潜力，可在数分钟内为数百万的用户带来真正的影响。\n",
    "\n",
    "## 3.目标\n",
    "* 我们唯一的目标是将数据集中的新闻分类为假新闻或真实新闻。\n",
    "* 新闻的细致EDA\n",
    "* 选择并建立强大的分类模型\n",
    "## 数据\n",
    "数据地址：[https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.PaddleNLP环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -U paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.解压缩数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 运行一次解压缩，后续注释掉\r\n",
    "# !unzip data/data27271/真假新闻数据集.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.导出基础库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 基本数据包：pandas和numpy\r\n",
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "import os\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "虚假新闻数据集的大小以及字段 (row, column):(23481, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n",
      "None\n",
      "\n",
      " --------------------------------------- \n",
      "\n",
      "真实新闻数据集的大小以及字段 (row, column):(21417, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    21417 non-null  object\n",
      " 1   text     21417 non-null  object\n",
      " 2   subject  21417 non-null  object\n",
      " 3   date     21417 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 669.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取数据集\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "true_news = pd.read_csv('True.csv')\n",
    "# 虚假新闻数据集的大小以及字段\n",
    "print (\"虚假新闻数据集的大小以及字段 (row, column):\"+ str(fake_news.shape))\n",
    "print (fake_news.info())\n",
    "print(\"\\n --------------------------------------- \\n\")\n",
    "# 真实新闻数据集的大小以及字段\n",
    "print (\"真实新闻数据集的大小以及字段 (row, column):\"+ str(true_news.shape))\n",
    "print (true_news.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.数据合并\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 标签转换\r\n",
    "true_news['label'] = 0\r\n",
    "fake_news['label'] = 1\r\n",
    "\r\n",
    "# 数据合并\r\n",
    "news_all = pd.concat([true_news, fake_news], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                    title  \\\n",
       "0      As U.S. budget fight looms, Republicans flip t...   \n",
       "1      U.S. military to accept transgender recruits o...   \n",
       "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3      FBI Russia probe helped by Australian diplomat...   \n",
       "4      Trump wants Postal Service to charge 'much mor...   \n",
       "...                                                  ...   \n",
       "44893  McPain: John McCain Furious That Iran Treated ...   \n",
       "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "44896  How to Blow $700 Million: Al Jazeera America F...   \n",
       "44897  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "0      WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1      WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "...                                                  ...           ...   \n",
       "44893  21st Century Wire says As 21WIRE reported earl...   Middle-east   \n",
       "44894  21st Century Wire says It s a familiar theme. ...   Middle-east   \n",
       "44895  Patrick Henningsen  21st Century WireRemember ...   Middle-east   \n",
       "44896  21st Century Wire says Al Jazeera America will...   Middle-east   \n",
       "44897  21st Century Wire says As 21WIRE predicted in ...   Middle-east   \n",
       "\n",
       "                     date  label  \n",
       "0      December 31, 2017       0  \n",
       "1      December 29, 2017       0  \n",
       "2      December 31, 2017       0  \n",
       "3      December 30, 2017       0  \n",
       "4      December 29, 2017       0  \n",
       "...                   ...    ...  \n",
       "44893    January 16, 2016      1  \n",
       "44894    January 16, 2016      1  \n",
       "44895    January 15, 2016      1  \n",
       "44896    January 14, 2016      1  \n",
       "44897    January 12, 2016      1  \n",
       "\n",
       "[44898 rows x 5 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_all.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 自定义reader方法\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddle.io import Dataset, Subset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "\r\n",
    "def read(pd_data):\r\n",
    "    for index, item in pd_data.iterrows():       \r\n",
    "        yield {'text': item['title']+'. '+item['text'], 'label': item['label'], 'qid': index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40408\n",
      "4490\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\r\n",
    "all_ds = load_dataset(read, pd_data=news_all,lazy=False)\r\n",
    "train_ds = Subset(dataset=all_ds, indices=[i for i in range(len(all_ds)) if i % 10 != 1])\r\n",
    "dev_ds = Subset(dataset=all_ds, indices=[i for i in range(len(all_ds)) if i % 10 == 1])\r\n",
    "\r\n",
    "# 在转换为MapDataset类型\r\n",
    "train_ds = MapDataset(train_ds)\r\n",
    "dev_ds = MapDataset(dev_ds)\r\n",
    "print(len(train_ds))\r\n",
    "print(len(dev_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、SKEP模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-26 00:43:18,015] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_2.0_large_en/skep_ernie_2.0_large_en.pdparams\n",
      "[2021-07-26 00:43:28,446] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_2.0_large_en/skep_ernie_2.0_large_en.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "# 指定模型名称，一键加载模型\r\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\", num_classes=2)\r\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\r\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、NLP数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.加入日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualdl引入\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "writer = LogWriter(\"./log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.SkepTokenizer数据处理\n",
    "SKEP模型对文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的SkepTokenizer完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader\r\n",
    "\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False):\r\n",
    "   \r\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\r\n",
    "\r\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        # label：情感极性类别\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        # qid：每条数据的编号\r\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 批量数据大小\r\n",
    "batch_size = 10\r\n",
    "# 文本序列最大长度\r\n",
    "max_seq_length = 512\r\n",
    "\r\n",
    "# 将数据处理成模型可读入的数据格式\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "# 将数据组成批量式数据，如\r\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "# 将每条数据label堆叠在一起\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型训练和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "\r\n",
    "from utils import evaluate\r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 3\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"skep_ckpt\"\r\n",
    "# len(train_data_loader)一轮训练所需要的step数\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# Adam优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=2e-5,\r\n",
    "    parameters=model.parameters())\r\n",
    "# 交叉熵损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "# accuracy评价指标\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        # 喂数据给model\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        # 计算损失函数值\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        # 预测分类概率值\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        # 计算acc\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 100 == 0:\r\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\r\n",
    "            if not os.path.exists(save_dir):\r\n",
    "                os.makedirs(save_dir)\r\n",
    "            # 评估当前训练的模型\r\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "            # 保存当前模型参数等\r\n",
    "            model.save_pretrained(save_dir)\r\n",
    "            # 保存tokenizer的词表等\r\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.训练日志\n",
    "```\n",
    "global step 110, epoch: 1, batch: 110, loss: 0.00653, accu: 0.98000, speed: 0.05 step/s\n",
    "global step 120, epoch: 1, batch: 120, loss: 0.00180, accu: 0.99000, speed: 0.95 step/s\n",
    "global step 130, epoch: 1, batch: 130, loss: 0.00236, accu: 0.99000, speed: 0.94 step/s\n",
    "global step 140, epoch: 1, batch: 140, loss: 0.00210, accu: 0.99250, speed: 0.94 step/s\n",
    "global step 150, epoch: 1, batch: 150, loss: 0.00216, accu: 0.99400, speed: 0.95 step/s\n",
    "global step 160, epoch: 1, batch: 160, loss: 0.00651, accu: 0.99500, speed: 0.95 step/s\n",
    "global step 170, epoch: 1, batch: 170, loss: 0.00105, accu: 0.99571, speed: 0.95 step/s\n",
    "global step 180, epoch: 1, batch: 180, loss: 0.00092, accu: 0.99625, speed: 0.94 step/s\n",
    "global step 190, epoch: 1, batch: 190, loss: 0.00065, accu: 0.99667, speed: 0.94 step/s\n",
    "global step 200, epoch: 1, batch: 200, loss: 0.00058, accu: 0.99700, speed: 0.95 step/s\n",
    "eval loss: 0.00571, accu: 0.99866\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、总结\n",
    "## 1.数据处理\n",
    "* 合并数据集\n",
    "* 生成新标签\n",
    "* 数据集划分\n",
    "## 2.PaddleNLP自定义reader方法\n",
    "以前多是直接读文件再返回，最近一直用pandas读取返回，更方便快捷\n",
    "## 3.SKEP模型应用\n",
    "分类好像就这一个模型，max_seq_length指的是单词最大数量，不超过512，如果超过了要用各种trick，比如前面截取、后面截取，诸如此类，总之会丢掉一部分。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
